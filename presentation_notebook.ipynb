{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e4bf28",
   "metadata": {},
   "source": [
    "# INTRO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3443e941-51bf-4d25-9d8e-01a08e066078",
   "metadata": {},
   "source": [
    "## Who am I?\n",
    "\n",
    "<img src=\"./assets/my_background.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a9a4d-aed6-4546-9895-bc9dccae5ecb",
   "metadata": {},
   "source": [
    "---\n",
    "## All about Accure\n",
    "\n",
    "<img src=\"./assets/USP.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "\n",
    "- Accure uses Battery expertise & cloud-powered software development to make batteries safer, longer-lasting, and cheaper\n",
    "- \"Spin off\" from the [RWTH Acchen ISEA Institute](https://www.isea.rwth-aachen.de/cms/ISEA/Forschung/Einrichtungen/~peqw/Labor-fuer-Batteriealterung/?lidx=1) in mid 2020\n",
    "- Now employs nearly 60 battery experts, software engineers, sales reps, ect...\n",
    "- [We're Hiring!](https://www.accure.net/careers)\n",
    "\n",
    "\n",
    "<img src=\"./assets/method.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023680ee-bb4a-45df-ba74-2e109aef2bab",
   "metadata": {},
   "source": [
    "---\n",
    "## Where Dask / Coiled fit in\n",
    "\n",
    "* Dask is great (but not perfect)\n",
    "* Coiled gives us Dask, so that's great\n",
    "* But we don't use Coiled for all Dask use-cases\n",
    "* And we don't exclusively use Dask\n",
    "\n",
    "<img src=\"./assets/stack.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4becf-ddc7-4fe3-907e-32a276680e4b",
   "metadata": {},
   "source": [
    "### Accure Utilities\n",
    "\n",
    "* **Accure-IO**: Input/output to data storages\n",
    "* **Accure-Log**: Cross-platform logging to ELK\n",
    "* **Accure-Events**: Rapid event-driven service development\n",
    "* **Accure-Distributed**: Distributed-computing framework\n",
    "    * \"On top\" of Dask\n",
    "    * Adds work avoidance\n",
    "    * Adds artifact management\n",
    "    * Adds failover contigencies\n",
    "    * Simplifies cross-platform execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff5daf-c38b-413f-b76f-82881f74f389",
   "metadata": {},
   "source": [
    "---\n",
    "# Task Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70747f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "from distributed import Client, LocalCluster\n",
    "from coiled import Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c39ad7-c0fc-4ac0-ac01-b0848d8cccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define an standard task framework\n",
    "class ScaledTask:\n",
    "    def __init__(\n",
    "        self, \n",
    "        failover: Callable,\n",
    "        # loader: Callable # Work Avoidance!\n",
    "    ):\n",
    "        self.failover = failover\n",
    "    \n",
    "    def __call__(self, func):\n",
    "        def wrapper(*args, task_name=\"no_task_name\", task_id='no_task_id', runtime=None, **kwargs):\n",
    "            ###\n",
    "            # Pre-set-up stuff in the \"Pipeline Worker\". For example...\n",
    "            #   - Entry logging\n",
    "            #   - Checking for duplicate Task-names / Task-IDs\n",
    "            #   - Waiting for Dask workers, if Dasky\n",
    "            #   - Checking for common inputs, scattering if needed\n",
    "            #   - Setup task-runtime\n",
    "            runtime = runtime or {}\n",
    "            quiet = runtime.get(\"quiet\", False)\n",
    "            \n",
    "            task_runtime = dict(\n",
    "                task_name=task_name,\n",
    "                task_id=task_id,\n",
    "                run_id = runtime.get(\"run_id\", 'no-run-id'),\n",
    "                product = runtime.get(\"product\", 'no-product'),\n",
    "                allow_failure = runtime.get(\"allow_failure\", False),\n",
    "                quiet=quiet\n",
    "            )\n",
    "            \n",
    "            if not quiet:\n",
    "                print(f\"{task_name}-{task_id}: Submitting\")\n",
    "            \n",
    "            ###\n",
    "            # Submit Task, and handle error\n",
    "            all_task_kwargs = dict(func_args=args, func_kwargs=kwargs, runtime=task_runtime)\n",
    "            \n",
    "            client = runtime.get('client', None)\n",
    "            if isinstance(client, Client):\n",
    "                # Execute in cluster\n",
    "                output = client.submit(_remote_execution, wrapped_func=func, failover=self.failover, **all_task_kwargs)\n",
    "            else:\n",
    "                # Execute locally\n",
    "                output = _remote_execution(wrapped_func=func, failover=self.failover, **all_task_kwargs)\n",
    "              \n",
    "            ###\n",
    "            # Do task clean up. For example...\n",
    "            #   - Writing artifacts to AWS S3 / RDS\n",
    "            #   - Exit logging\n",
    "            if not quiet:\n",
    "                print(f\"{task_name}-{task_id}: Cleaning up\")\n",
    "            \n",
    "            # All done!\n",
    "            return output\n",
    "        return wrapper\n",
    "    \n",
    "def _remote_execution(wrapped_func, failover, func_args, func_kwargs, runtime):\n",
    "    # Pre-set-up stuff in the \"Remote Worker\". For example...\n",
    "    #   - Checking for artifacts on local / S3 (work avoidance!)\n",
    "    #   - Determine recomputation needs\n",
    "    #   - Open a tempdir to place local side-effects\n",
    "    #   - Entry logging\n",
    "    #   - Checking for common inputs, scattering if needed\n",
    "    quiet = runtime.get(\"quiet\", False)\n",
    "    if not quiet:\n",
    "        print(f\"{runtime['task_name']}-{runtime['task_id']}: Executing\")\n",
    "    \n",
    "    ###\n",
    "    # Invoke function\n",
    "    try:\n",
    "        output = wrapped_func(*func_args, runtime=runtime, **func_kwargs)\n",
    "    except Exception as exc:\n",
    "        # Do standard logging!\n",
    "        if not quiet:\n",
    "            print(f\"{runtime['task_name']}-{runtime['task_id']}: Failure\")\n",
    "\n",
    "        if not runtime[\"allow_failure\"]:\n",
    "            raise  exc\n",
    "        else:\n",
    "            output = failover(*func_args, runtime=runtime, **func_kwargs)\n",
    "    else:\n",
    "        if not quiet:\n",
    "            print(f\"{runtime['task_name']}-{runtime['task_id']}: Success\")\n",
    "    \n",
    "    ###\n",
    "    # Do Remote cleanup\n",
    "    #  - Delete any local side effects (tempdir)\n",
    "    #  - Writing artifacts to S3\n",
    "    #  - Exit logging\n",
    "    if not quiet:\n",
    "        print(f\"{runtime['task_name']}-{runtime['task_id']}: Finished\")\n",
    "    \n",
    "    ###\n",
    "    # Done!\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ce35a-19a9-49d3-ab70-d4ca99172106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_a_thing_failover(arg:int, runtime:Dict)->pd.DataFrame:\n",
    "    \"\"\"Something which runs when the 'real' task fails, and returns some meaningful response which fits\"\"\"\n",
    "    return pd.DataFrame({\"a\":[np.nan], \"b\":[\"so sad :(\"]})\n",
    "\n",
    "@ScaledTask(failover=do_a_thing_failover)\n",
    "def do_a_thing(arg:int, runtime:Dict)->pd.DataFrame:\n",
    "    # pretend to work reeealy hard ;)\n",
    "    time.sleep(arg%10+1)\n",
    "    \n",
    "    # Simulate a failure\n",
    "    if (arg)%7==0:\n",
    "        raise RuntimeError(\"Everything is terrible\")\n",
    "    \n",
    "    # Formulate an output\n",
    "    data = pd.DataFrame({\"a\":[arg], \"b\":[\"hi\" if arg%2==0 else \"boo\"]})\n",
    "    \n",
    "    # Return like normal\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c3604-b4f6-4c70-835a-3f7e1f5bd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple execution\n",
    "do_a_thing(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82224f88-2a89-405c-9180-4450a08f5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failing execution\n",
    "do_a_thing(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134f2d0-171a-41f0-95c6-ddeb3543c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failing execution, but with failover\n",
    "do_a_thing(7, runtime={\"allow_failure\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ae74a7-7834-4171-a754-77c935d3844e",
   "metadata": {},
   "source": [
    "---\n",
    "# Generalize Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f840e-f4ab-4d09-a5e6-d9d593009a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define an standard pipeline framework\n",
    "class ScaledPipeline:\n",
    "    def __init__(self, product):\n",
    "        self.product=product\n",
    "    \n",
    "    def __call__(self, func):\n",
    "        def wrapper(*args, cluster=None, runtime:Dict=None, **kwargs):\n",
    "            # Pre-set-up stuff. For example...\n",
    "            #   - Setting defaults\n",
    "            #   - Initializing (or conencting to) dask clusters\n",
    "            #   - Adapting to run-context\n",
    "            #   - Setup logging\n",
    "            run_id = \"random_badger_14\"\n",
    "            runtime = runtime or {}\n",
    "            runtime.update(dict(\n",
    "                client = Client(cluster) if cluster else None,\n",
    "                run_id = run_id,\n",
    "                product = self.product\n",
    "            ))\n",
    "            \n",
    "            # Execute Pipeline\n",
    "            output = func(*args, runtime=runtime, **kwargs)\n",
    "            \n",
    "            # Do pipeline clean up. For example...\n",
    "            #   - Ensuring worker shutdown\n",
    "            #   - Writing artifacts to AWS S3 / RDS\n",
    "            print(\"Cleaning up...\")\n",
    "            \n",
    "            # All done!\n",
    "            return output\n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395e373-bb89-4d3c-9153-15ef92837a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_futures(futures:List[pd.DataFrame], runtime:Dict=None, gather_chunking=50)->pd.DataFrame:\n",
    "    \"\"\"Aggregates many futures into one. In case there's alot, use a cascading algorithm\"\"\"\n",
    "    runtime = runtime or {}\n",
    "    client = runtime.get(\"client\", None)\n",
    "    if client is None:\n",
    "        # No Dask, just concat like normal\n",
    "        return pd.concat(futures, ignore_index=True)\n",
    "        \n",
    "    if len(futures) <= gather_chunking:\n",
    "        # Yes Dask, but few futures. Concat in a job\n",
    "        return client.submit(pd.concat, futures, ignore_index=True)\n",
    "    \n",
    "    # Yes Dask, and lot's of futures. Gather cascade!\n",
    "    n_chunks = len(futures) // gather_chunking\n",
    "    if len(futures) % gather_chunking > 0:\n",
    "        n_chunks+=1\n",
    "    \n",
    "    grouped_future_set = [\n",
    "        futures[\n",
    "            i*gather_chunking: (i+1)*gather_chunking\n",
    "        ] for i in range(n_chunks)\n",
    "    ]\n",
    "\n",
    "    recursive_futures = [\n",
    "        client.submit(\n",
    "            pd.concat, \n",
    "            grouped_futures,\n",
    "            ignore_index=True\n",
    "        ) for grouped_futures in grouped_future_set\n",
    "    ]\n",
    "\n",
    "    return aggregate_futures(recursive_futures, runtime)\n",
    "\n",
    "def gather(future, runtime:Dict=None):\n",
    "    \"\"\"Calles client.gather, if one is available, otherwise returns `future` directly\"\"\"\n",
    "    runtime = runtime or {}\n",
    "    client = runtime.get(\"client\", None)\n",
    "    if client is None:\n",
    "        return future\n",
    "    \n",
    "    return client.gather(future)\n",
    "    \n",
    "\n",
    "\n",
    "@ScaledPipeline(product= \"nice-product\")\n",
    "def do_a_lot_of_things(n_ids:int=10, runtime:Dict=None):\n",
    "    print(\"Pipeline: Beginning\")\n",
    "    runtime = runtime or {}    \n",
    "    \n",
    "    # Iterate over a list of things and store multiple \"futures\"\n",
    "    print(\"Pipeline: Creating futures\")\n",
    "    futures = [\n",
    "        do_a_thing(\n",
    "            arg=i,                    \n",
    "            task_name=\"do_a_thing\",                \n",
    "            task_id=i,\n",
    "            runtime=runtime\n",
    "        ) for i in range(n_ids)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Pass all of those futures into a single task, and deal with them \n",
    "    #  - We only run this task once in this pipeline, so it's okay to not specify a `task_name` (i.e. defaults to the function name) or `task_id` \n",
    "    print(\"Pipeline: Combining futures\")\n",
    "    combined_future = aggregate_futures(futures, runtime)\n",
    "\n",
    "    # Call runtime.gather to collect the results of the calculations\n",
    "    print(\"Pipeline: Gathering final future\")\n",
    "    data = gather(combined_future, runtime)\n",
    "    \n",
    "    # Return the overall result\n",
    "    print(\"Pipeline: Finshed\")\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64560dc-1eb1-4e3e-89a4-9b5c3fbb4a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_a_lot_of_things(5, runtime={\"allow_failure\":True} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c116334-99cf-4fa3-a107-3285d7457c1e",
   "metadata": {},
   "source": [
    "---\n",
    "# Submit pipeline via Local Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571c2a92-e4a8-424a-9d7d-db517bc71f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=4, threads_per_worker=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b27b32-6021-49fe-854f-634bed86cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_a_lot_of_things(5, runtime={\"allow_failure\":True, \"quiet\":True}, cluster=cluster )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dedefa-b394-43c8-bda2-0efcb520fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803f0d3-8bdf-4113-b68b-fa5531b53484",
   "metadata": {},
   "source": [
    "---\n",
    "# Submit pipeline via Coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291bad3-8bfc-4a42-8914-a5c51abd7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! Make sure to insert 'YOUR_COMPANY_ACCOUNT'\n",
    "\n",
    "coiled_cluster = Cluster(\n",
    "    name=\"sevberg-webinar-cluster\",\n",
    "    software=\"coiled/default-py38\",\n",
    "    account=\"YOUR_COMPANY_ACCOUNT\",\n",
    "    n_workers=5,\n",
    "    worker_vm_types=[\"m5.xlarge\"],\n",
    "    worker_options={\"nthreads\": 40},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f983a833-a430-4c4e-a382-f86ede7ff902",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_a_lot_of_things(200, runtime={\"allow_failure\":True, \"quiet\":True}, cluster=coiled_cluster )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1875b22-577e-4807-9af2-1a249ecf99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "coiled_cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "de6f33218722a7d60b43d2509d962870fe1b8d3e624f130f45b2062a1b8e6150"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
